{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to read training images\n",
      "Now going to read dogs files (Index: 0)\n",
      "Now going to read cats files (Index: 1)\n",
      "Complete reading input data. Will Now print a snippet of it\n",
      "Number of files in Training-set:\t\t800\n",
      "Number of files in Validation-set:\t200\n",
      "WARNING:tensorflow:From <ipython-input-1-52c8d94266d4>:42: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import dataset\n",
    "\n",
    "#Adding Seed so that random initialization is consistent\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#Prepare input data\n",
    "classes = ['dogs','cats']\n",
    "num_classes = len(classes)\n",
    "\n",
    "# 20% of the data will automatically be used for validation\n",
    "validation_size = 0.2\n",
    "img_size = 128\n",
    "num_channels = 3\n",
    "train_path='training_data'\n",
    "\n",
    "# We shall load all the training and validation images and labels into memory using openCV and use that during training\n",
    "data = dataset.read_train_sets(train_path, img_size, classes, validation_size=validation_size)\n",
    "\n",
    "\n",
    "print(\"Complete reading input data. Will Now print a snippet of it\")\n",
    "print(\"Number of files in Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
    "print(\"Number of files in Validation-set:\\t{}\".format(len(data.valid.labels)))\n",
    "\n",
    "\n",
    "session = tf.Session()\n",
    "X = tf.placeholder(tf.float32, shape=[None, img_size,img_size,num_channels], name='x')\n",
    "\n",
    "## labels\n",
    "Y_true = tf.placeholder(tf.float32, shape=[None, 2], name='y_true')\n",
    "Y_true_cls = tf.argmax(Y_true, dimension=1)\n",
    "\n",
    "global_step =10\n",
    "\n",
    "def create_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def create_biases(shape):\n",
    "    return tf.Variable(tf.constant(0.05, shape=shape))\n",
    "\n",
    "def conv2d(X,W):\n",
    "    return tf.mm.conv2d(X,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "def max_pool_2x2(X):\n",
    "    return tf.nn.max_pool(X,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1=create_weights([3,3,3,32])\n",
    "b_conv1=create_biases([32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1=tf.nn.conv2d(X,W_conv1,strides=[1,1,1,1],padding='SAME')\n",
    "layer1= layer1 + b_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_max_pool1=tf.nn.max_pool(layer1,ksize=[1,2,2,1],strides =[1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_relu1=tf.nn.relu(layer_max_pool1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2=create_weights([3,3,32,32])\n",
    "b_conv2=create_biases([32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2=tf.nn.conv2d(layer_relu1,W_conv2,strides=[1,1,1,1],padding='SAME')\n",
    "layer2= layer2+b_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_max_pool2=tf.nn.max_pool(layer2,ksize=[1,2,2,1],strides =[1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_relu2=tf.nn.relu(layer_max_pool2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 64)\n"
     ]
    }
   ],
   "source": [
    "W_conv3=create_weights([3,3,32,64])\n",
    "b_conv3=create_biases([64])\n",
    "layer3=tf.nn.conv2d(layer_relu2,W_conv3,strides=[1,1,1,1],padding='SAME')\n",
    "layer3= layer3+b_conv3\n",
    "layer_max_pool3=tf.nn.max_pool(layer3,ksize=[1,2,2,1],strides =[1,2,2,1],padding='SAME')\n",
    "layer_relu3=tf.nn.relu(layer_max_pool3)\n",
    "print(layer_relu3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16384)\n"
     ]
    }
   ],
   "source": [
    "fc1_flat = tf.reshape (layer_relu3,[-1,16*16*64])\n",
    "print(fc1_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16384, 128)\n"
     ]
    }
   ],
   "source": [
    "weights_fc1 = create_weights([fc1_flat.get_shape()[1:4].num_elements(),128])\n",
    "print(weights_fc1.shape)\n",
    "biases_fc1= create_weights([128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128)\n"
     ]
    }
   ],
   "source": [
    "fc1 = tf.matmul(fc1_flat, weights_fc1) + biases_fc1\n",
    "print(fc1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_fc2 = create_weights([fc1.get_shape()[1:4].num_elements(),2])\n",
    "biases_fc2= create_weights([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc2 = tf.matmul(fc1, weights_fc2) + biases_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =tf.nn.softmax(fc2, name='y_pred')\n",
    "y_pred_cls=tf.argmax(y_pred, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy= tf.nn.softmax_cross_entropy_with_logits(logits=fc2, labels=Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=tf.reduce_mean(cross_entropy)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction=tf.equal(y_pred_cls,Y_true_cls)\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1 --- Training Accuracy:  53.1%, Validation Accuracy:  46.9%,  Validation Loss: 0.739\n",
      "Training Epoch 2 --- Training Accuracy:  65.6%, Validation Accuracy:  56.2%,  Validation Loss: 0.689\n",
      "Training Epoch 3 --- Training Accuracy:  56.2%, Validation Accuracy:  53.1%,  Validation Loss: 0.683\n",
      "Training Epoch 4 --- Training Accuracy:  65.6%, Validation Accuracy:  59.4%,  Validation Loss: 0.687\n",
      "Training Epoch 5 --- Training Accuracy:  71.9%, Validation Accuracy:  43.8%,  Validation Loss: 0.746\n",
      "Training Epoch 6 --- Training Accuracy:  62.5%, Validation Accuracy:  43.8%,  Validation Loss: 0.736\n",
      "Training Epoch 7 --- Training Accuracy:  62.5%, Validation Accuracy:  56.2%,  Validation Loss: 0.672\n",
      "Training Epoch 8 --- Training Accuracy:  62.5%, Validation Accuracy:  68.8%,  Validation Loss: 0.625\n",
      "Training Epoch 9 --- Training Accuracy:  81.2%, Validation Accuracy:  62.5%,  Validation Loss: 0.655\n",
      "Training Epoch 10 --- Training Accuracy:  81.2%, Validation Accuracy:  65.6%,  Validation Loss: 0.663\n",
      "Training Epoch 11 --- Training Accuracy:  87.5%, Validation Accuracy:  46.9%,  Validation Loss: 0.770\n",
      "Training Epoch 12 --- Training Accuracy:  87.5%, Validation Accuracy:  50.0%,  Validation Loss: 0.740\n",
      "Training Epoch 13 --- Training Accuracy:  87.5%, Validation Accuracy:  68.8%,  Validation Loss: 0.609\n",
      "Training Epoch 14 --- Training Accuracy:  87.5%, Validation Accuracy:  75.0%,  Validation Loss: 0.597\n",
      "Training Epoch 15 --- Training Accuracy:  87.5%, Validation Accuracy:  62.5%,  Validation Loss: 0.629\n",
      "Training Epoch 16 --- Training Accuracy:  87.5%, Validation Accuracy:  68.8%,  Validation Loss: 0.600\n",
      "Training Epoch 17 --- Training Accuracy:  90.6%, Validation Accuracy:  53.1%,  Validation Loss: 0.828\n",
      "Training Epoch 18 --- Training Accuracy:  90.6%, Validation Accuracy:  56.2%,  Validation Loss: 0.779\n",
      "Training Epoch 19 --- Training Accuracy:  87.5%, Validation Accuracy:  71.9%,  Validation Loss: 0.552\n",
      "Training Epoch 20 --- Training Accuracy:  87.5%, Validation Accuracy:  71.9%,  Validation Loss: 0.628\n",
      "Training Epoch 21 --- Training Accuracy:  93.8%, Validation Accuracy:  62.5%,  Validation Loss: 0.638\n",
      "Training Epoch 22 --- Training Accuracy:  93.8%, Validation Accuracy:  75.0%,  Validation Loss: 0.619\n",
      "Training Epoch 23 --- Training Accuracy:  93.8%, Validation Accuracy:  53.1%,  Validation Loss: 0.920\n",
      "Training Epoch 24 --- Training Accuracy:  93.8%, Validation Accuracy:  56.2%,  Validation Loss: 0.892\n",
      "Training Epoch 25 --- Training Accuracy:  90.6%, Validation Accuracy:  78.1%,  Validation Loss: 0.549\n",
      "Training Epoch 26 --- Training Accuracy:  90.6%, Validation Accuracy:  68.8%,  Validation Loss: 0.682\n",
      "Training Epoch 27 --- Training Accuracy:  93.8%, Validation Accuracy:  65.6%,  Validation Loss: 0.644\n",
      "Training Epoch 28 --- Training Accuracy:  93.8%, Validation Accuracy:  75.0%,  Validation Loss: 0.721\n",
      "Training Epoch 29 --- Training Accuracy:  96.9%, Validation Accuracy:  53.1%,  Validation Loss: 1.041\n",
      "Training Epoch 30 --- Training Accuracy:  96.9%, Validation Accuracy:  56.2%,  Validation Loss: 0.999\n",
      "Training Epoch 31 --- Training Accuracy:  96.9%, Validation Accuracy:  78.1%,  Validation Loss: 0.575\n",
      "Training Epoch 32 --- Training Accuracy:  96.9%, Validation Accuracy:  68.8%,  Validation Loss: 0.751\n",
      "Training Epoch 33 --- Training Accuracy:  96.9%, Validation Accuracy:  65.6%,  Validation Loss: 0.676\n",
      "Training Epoch 34 --- Training Accuracy:  96.9%, Validation Accuracy:  62.5%,  Validation Loss: 0.890\n",
      "Training Epoch 35 --- Training Accuracy:  96.9%, Validation Accuracy:  50.0%,  Validation Loss: 1.232\n",
      "Training Epoch 36 --- Training Accuracy:  96.9%, Validation Accuracy:  53.1%,  Validation Loss: 1.190\n",
      "Training Epoch 37 --- Training Accuracy:  96.9%, Validation Accuracy:  75.0%,  Validation Loss: 0.636\n"
     ]
    }
   ],
   "source": [
    "train_path='training_data'\n",
    "\n",
    "\n",
    "# Merge all summaries\n",
    "#merged_summary_op  = tf.summary.merge_all()\n",
    "\n",
    "session.run(tf.global_variables_initializer()) \n",
    "\n",
    "batch_size = 32\n",
    "total_iterations = 0\n",
    "train_batch_size = 32\n",
    "\n",
    "x_batch, y_true_batch, _, cls_batch = data.train.next_batch(batch_size)\n",
    "\n",
    "feed_dict_train = {X: x_batch, Y_true: y_true_batch}\n",
    "\n",
    "session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "\n",
    "x_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(train_batch_size)\n",
    "\n",
    "feed_dict_val = {X: x_valid_batch, Y_true: y_valid_batch}\n",
    "\n",
    "val_loss = session.run(cost, feed_dict=feed_dict_val)\n",
    "\n",
    "def show_progress(epoch, feed_dict_train, feed_dict_validate, val_loss):\n",
    "    acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "    val_acc = session.run(accuracy, feed_dict=feed_dict_validate)\n",
    "    msg = \"Training Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%},  Validation Loss: {3:.3f}\"\n",
    "    print(msg.format(epoch + 1, acc, val_acc, val_loss))\n",
    "\n",
    "\n",
    "def train(num_iteration):\n",
    "    global total_iterations\n",
    "\n",
    "    for i in range(total_iterations, total_iterations + num_iteration):\n",
    "\n",
    "        x_batch, y_true_batch, _, cls_batch = data.train.next_batch(batch_size)\n",
    "        x_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(batch_size)\n",
    "\n",
    "        feed_dict_tr = {X: x_batch, Y_true: y_true_batch}\n",
    "        feed_dict_val = {X: x_valid_batch, Y_true: y_valid_batch}\n",
    "\n",
    "        session.run(optimizer, feed_dict=feed_dict_tr)\n",
    "\n",
    "        if i % int(data.train.num_examples / batch_size) == 0:\n",
    "            val_loss = session.run(cost, feed_dict=feed_dict_val)\n",
    "            epoch = int(i / int(data.train.num_examples / batch_size))\n",
    "\n",
    "            show_progress(epoch, feed_dict_tr, feed_dict_val, val_loss)\n",
    "            #saver.save(session, 'dogs-cats-model')\n",
    "\n",
    "    total_iterations += num_iteration\n",
    "\n",
    " \n",
    "train(num_iteration=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
